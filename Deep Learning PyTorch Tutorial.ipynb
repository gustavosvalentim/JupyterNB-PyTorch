{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`autograd` package provides automatic differentiation for all operations on Tensors. It is a define-by-run framework.\n",
    "If you set `.requires_grad` as `True`, it starts to track automatically every operations on the Tensor, so when you call `.backward()` you have all the gradients computed automatically. To prevent future history tracking, you can call `.detach()`, to detach it from computation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.ones(5, 3, requires_grad = True)\n",
    "print(tensor1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "tensor2 = tensor1 + 2\n",
    "print(tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x7f25a49ef690>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.],\n",
      "        [5., 5., 5.],\n",
      "        [5., 5., 5.],\n",
      "        [5., 5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(tensor1 + 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torchvision package contains datasets like MNIST to be loaded in runtime. It also have transformation functions to transform data coming from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/raw/train-images-idx3-ubyte.gz to MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/raw/train-labels-idx1-ubyte.gz to MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train = datasets.MNIST(\"\", train = True, download = True, transform = transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.MNIST(\"\", train = False, download = True, transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(train, batch_size = 10, shuffle = True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size = 10, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*testset* and *trainset* are arrays of arrays, containg data we have in the original dataset but separeted by batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([8, 0, 1, 2, 8, 4, 0, 6, 1, 2])]\n"
     ]
    }
   ],
   "source": [
    "for d in testset:\n",
    "    print(d)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4edd039090>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = []\n",
    "for d in testset:\n",
    "    print(d[1][0])\n",
    "    image = d[0][0]\n",
    "    break\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# by default the image comes as [1, 28, 28] shaping, so we need to make it be [28, 28] because the image have the dimension 28x28\n",
    "imagereshape = image.view(28, 28)\n",
    "plt.imshow(imagereshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the class `Net` will be the Neural Network. Initializing *nn.Module*, our *Net* class will inherit the methods and attributes.\n",
    "\n",
    "In the `__init__` method we will define the fully connected layers.\n",
    "\n",
    "`nn.Linear` accepts two parameters, the first one is the input, for the first layer (the input layer), we will set the `input` parameter to `28 * 28` because our image have dimension of `28x28`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Initialize nn.Module\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        inp = F.relu(self.fc1(inp))\n",
    "        inp = F.relu(self.fc2(inp))\n",
    "        inp = F.relu(self.fc3(inp))\n",
    "        \n",
    "        return F.log_softmax(self.fc4(inp), dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rand_t` is a tensor of *28x28* with random numbers in it.\n",
    "\n",
    "after creating the tensor, we have to reshape it to be flat, the first parameters in `rand_t.view()` is very important, it says that we are flattening the multidimensional array of `28 x 28` to a single dimension array of `784` items, or 28 * 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.0518e-02, 4.3279e-01, 9.3872e-01, 4.1962e-01, 6.2357e-01, 2.9021e-01,\n",
      "         2.6606e-01, 2.8769e-01, 6.3534e-01, 9.3298e-01, 2.8463e-02, 3.9194e-01,\n",
      "         5.6457e-01, 5.0258e-02, 9.8753e-01, 4.8648e-01, 7.0875e-01, 9.5541e-01,\n",
      "         7.9751e-01, 9.6972e-02, 9.5851e-01, 8.5222e-01, 3.2154e-02, 1.2424e-01,\n",
      "         2.2151e-01, 1.5461e-01, 4.1052e-01, 2.0070e-01, 5.9621e-01, 2.0963e-01,\n",
      "         9.2633e-01, 9.0746e-01, 4.8457e-02, 8.4984e-01, 2.7668e-01, 3.4002e-01,\n",
      "         9.6983e-01, 4.3146e-01, 4.4559e-01, 1.1741e-01, 1.5673e-01, 3.6769e-01,\n",
      "         6.6580e-01, 7.9854e-01, 8.2705e-02, 4.9180e-01, 5.7894e-01, 2.2023e-01,\n",
      "         1.2759e-01, 7.9538e-01, 8.2174e-01, 3.6891e-01, 8.0748e-01, 7.8403e-01,\n",
      "         4.4740e-01, 8.2347e-02, 1.0628e-01, 3.0753e-02, 8.0374e-01, 4.8773e-01,\n",
      "         7.7784e-01, 9.8443e-01, 2.0135e-01, 5.7285e-01, 6.4198e-01, 2.5696e-01,\n",
      "         7.1533e-01, 7.4506e-01, 5.1155e-01, 8.9545e-01, 9.4675e-01, 9.9355e-01,\n",
      "         1.9828e-01, 3.0880e-01, 3.5358e-01, 6.4905e-01, 8.5124e-01, 7.2250e-02,\n",
      "         4.1084e-01, 1.7256e-01, 3.5402e-01, 6.6739e-02, 3.1773e-01, 7.3443e-01,\n",
      "         4.6716e-01, 2.3989e-01, 7.6719e-01, 4.8685e-01, 1.2571e-02, 5.0216e-01,\n",
      "         9.4358e-01, 4.1771e-01, 8.3578e-01, 4.2265e-01, 5.0397e-01, 1.6642e-01,\n",
      "         9.9503e-01, 3.7765e-01, 6.0813e-01, 9.4289e-02, 3.5059e-01, 5.2177e-01,\n",
      "         5.3906e-01, 2.7234e-01, 7.8726e-01, 5.1877e-01, 2.5054e-01, 9.8915e-01,\n",
      "         2.4286e-01, 2.5938e-01, 7.9511e-02, 5.7569e-01, 6.0036e-01, 8.3788e-01,\n",
      "         1.2983e-01, 5.9067e-01, 7.4290e-01, 9.7589e-01, 6.3599e-01, 2.4129e-01,\n",
      "         1.0640e-01, 1.7448e-01, 4.2117e-01, 9.6543e-01, 1.5434e-01, 2.8038e-01,\n",
      "         6.0749e-01, 2.1934e-01, 4.0079e-01, 4.6897e-01, 4.3017e-01, 5.9237e-02,\n",
      "         1.0942e-01, 1.9275e-01, 9.2874e-01, 9.3523e-01, 6.5560e-01, 9.3372e-01,\n",
      "         7.1724e-01, 9.3943e-01, 1.7416e-01, 7.2074e-01, 1.0343e-01, 1.6713e-01,\n",
      "         3.2953e-02, 8.3013e-01, 6.3351e-01, 1.6572e-01, 2.5777e-01, 8.2058e-01,\n",
      "         1.2969e-02, 1.6963e-01, 4.9950e-01, 4.8876e-01, 5.1133e-01, 5.4348e-01,\n",
      "         5.8047e-01, 7.5715e-01, 4.8573e-01, 6.8691e-01, 8.4490e-01, 2.5854e-01,\n",
      "         2.1672e-01, 1.8172e-01, 6.9820e-01, 6.2233e-01, 1.6314e-02, 2.3965e-01,\n",
      "         4.2730e-01, 2.5925e-01, 4.1882e-01, 9.0367e-01, 4.9169e-01, 4.3408e-01,\n",
      "         3.4852e-01, 3.8168e-02, 9.3922e-01, 6.8582e-01, 2.1024e-01, 9.4076e-01,\n",
      "         9.7828e-01, 6.7435e-01, 6.8425e-01, 7.7460e-02, 3.7125e-01, 6.8642e-01,\n",
      "         7.0191e-02, 5.3226e-01, 3.2839e-01, 4.7369e-01, 5.7178e-01, 7.7776e-01,\n",
      "         3.1242e-02, 3.0482e-01, 4.6732e-01, 2.2781e-01, 7.3747e-02, 1.2552e-01,\n",
      "         8.3270e-01, 4.9420e-01, 9.9816e-01, 8.8467e-01, 3.2777e-01, 9.2883e-01,\n",
      "         9.3161e-01, 9.4984e-02, 8.7710e-01, 8.4325e-01, 7.7440e-01, 2.2853e-02,\n",
      "         6.7835e-01, 9.7319e-01, 2.8501e-03, 1.2429e-01, 1.9274e-01, 9.1965e-01,\n",
      "         7.9403e-01, 7.6804e-01, 1.4084e-01, 5.4275e-02, 7.1383e-01, 1.2679e-01,\n",
      "         3.5776e-01, 3.8958e-01, 2.8071e-01, 1.8079e-01, 7.6521e-01, 4.2651e-01,\n",
      "         1.0667e-01, 8.3253e-01, 7.4918e-01, 2.5485e-02, 4.8531e-01, 1.0433e-01,\n",
      "         5.6330e-01, 1.9229e-01, 8.3106e-01, 5.0017e-01, 1.9059e-01, 7.6332e-01,\n",
      "         3.1993e-01, 6.4002e-01, 1.9738e-01, 1.9911e-01, 2.4321e-01, 9.8947e-01,\n",
      "         1.2475e-01, 8.2272e-01, 8.2658e-01, 9.6827e-01, 6.4878e-01, 7.6758e-01,\n",
      "         9.5453e-01, 2.9194e-01, 8.5655e-01, 3.7822e-01, 8.7971e-01, 8.3216e-01,\n",
      "         1.8881e-01, 3.7012e-01, 3.8871e-01, 8.4282e-01, 3.7863e-01, 5.4252e-01,\n",
      "         6.9278e-01, 1.8118e-01, 5.6430e-01, 1.7021e-01, 9.5968e-01, 3.3928e-01,\n",
      "         6.4889e-01, 4.7628e-01, 7.0344e-01, 8.1965e-01, 6.9188e-01, 9.7795e-02,\n",
      "         8.3211e-01, 6.6533e-01, 1.8441e-02, 6.3018e-01, 4.5946e-01, 5.4405e-01,\n",
      "         2.9720e-01, 2.6664e-01, 5.9175e-01, 4.5752e-01, 9.0632e-01, 2.0032e-01,\n",
      "         3.2304e-01, 2.5435e-01, 5.3674e-01, 7.9337e-01, 3.7487e-01, 1.5927e-01,\n",
      "         7.6669e-01, 1.2973e-01, 9.6709e-02, 6.3066e-01, 7.2992e-01, 8.6103e-01,\n",
      "         6.2353e-01, 9.0468e-01, 1.5128e-01, 5.9959e-01, 6.8379e-02, 9.2262e-01,\n",
      "         6.9561e-01, 6.1947e-01, 9.5235e-02, 1.0860e-01, 7.5784e-01, 3.1669e-01,\n",
      "         1.7193e-01, 5.2235e-01, 4.0029e-01, 1.6247e-01, 8.0645e-01, 6.8578e-01,\n",
      "         2.5492e-01, 3.1828e-01, 1.1315e-01, 9.0469e-01, 9.3057e-01, 2.0409e-01,\n",
      "         8.7892e-01, 5.7104e-01, 8.8481e-01, 8.0981e-01, 1.4915e-01, 1.8073e-02,\n",
      "         4.2567e-01, 4.5117e-01, 8.3919e-01, 8.8221e-01, 3.2755e-01, 4.7402e-01,\n",
      "         4.0744e-01, 1.5066e-01, 2.7627e-01, 4.2452e-01, 3.5484e-01, 2.7049e-01,\n",
      "         8.9623e-01, 6.9226e-01, 2.6167e-01, 4.0708e-01, 7.3104e-01, 7.3277e-02,\n",
      "         2.1713e-01, 8.7897e-01, 7.9457e-01, 6.9542e-01, 1.9513e-01, 2.2031e-01,\n",
      "         8.2256e-01, 7.2026e-01, 3.0259e-01, 3.1928e-01, 6.5986e-01, 7.3560e-02,\n",
      "         1.9127e-01, 4.7686e-01, 7.0522e-01, 7.6272e-01, 5.5303e-01, 9.0263e-01,\n",
      "         1.9712e-01, 8.2718e-01, 3.9893e-01, 1.7181e-01, 9.5437e-01, 9.0148e-01,\n",
      "         3.1055e-02, 2.2088e-02, 1.9532e-01, 1.7605e-01, 6.1495e-01, 6.0159e-01,\n",
      "         2.9465e-01, 8.7640e-01, 6.0697e-01, 7.5207e-01, 6.8119e-01, 7.8516e-01,\n",
      "         3.3423e-02, 5.6368e-01, 5.6092e-01, 1.9232e-01, 8.5734e-01, 2.5906e-01,\n",
      "         8.6253e-02, 9.6490e-01, 9.1342e-01, 4.9555e-01, 3.4511e-01, 6.5490e-01,\n",
      "         1.4989e-01, 1.3299e-01, 7.0701e-01, 2.2412e-01, 4.7494e-01, 5.5562e-01,\n",
      "         8.4291e-01, 2.0441e-02, 7.3029e-01, 1.4858e-01, 6.0813e-01, 3.4118e-01,\n",
      "         5.2572e-01, 2.3080e-01, 6.1484e-01, 7.7749e-01, 6.5204e-01, 4.3462e-01,\n",
      "         3.1437e-01, 6.0378e-01, 4.5402e-01, 5.7916e-01, 8.8962e-01, 1.2165e-01,\n",
      "         1.9119e-01, 7.4952e-02, 2.5323e-01, 6.2625e-01, 6.1339e-01, 1.1711e-01,\n",
      "         6.1314e-01, 2.1130e-01, 9.1984e-01, 4.1746e-01, 7.5552e-01, 9.4880e-01,\n",
      "         1.7248e-01, 7.6523e-01, 5.7134e-01, 3.5696e-01, 5.3913e-02, 5.2402e-01,\n",
      "         5.8663e-01, 5.5485e-01, 9.9821e-02, 6.0827e-01, 8.6838e-01, 2.0551e-01,\n",
      "         9.1112e-01, 2.0684e-01, 3.3076e-01, 4.2669e-01, 7.1496e-01, 7.6923e-01,\n",
      "         6.5623e-01, 3.8916e-01, 1.1183e-01, 6.1201e-01, 3.1919e-01, 5.4958e-01,\n",
      "         5.8282e-01, 4.2384e-01, 3.8779e-01, 7.1700e-01, 7.0837e-02, 9.7591e-01,\n",
      "         9.2670e-01, 4.8741e-01, 4.4102e-01, 5.9459e-01, 4.2521e-01, 9.2011e-01,\n",
      "         4.4566e-03, 6.9126e-01, 9.9078e-01, 5.2331e-01, 6.1444e-01, 9.7572e-01,\n",
      "         6.1769e-01, 7.8270e-01, 5.1429e-01, 6.4355e-01, 1.6982e-01, 7.3000e-01,\n",
      "         2.4283e-01, 6.5200e-01, 9.9665e-01, 7.8738e-01, 9.1202e-01, 6.2959e-01,\n",
      "         2.1570e-02, 3.8366e-01, 9.1960e-01, 9.7414e-01, 8.7104e-02, 5.3468e-01,\n",
      "         6.1062e-01, 4.0118e-01, 9.9562e-01, 7.0486e-02, 4.1332e-01, 3.1155e-01,\n",
      "         7.7521e-01, 2.5472e-01, 9.1961e-01, 9.9058e-01, 8.3155e-01, 3.1632e-04,\n",
      "         1.7023e-01, 6.6790e-01, 9.3679e-01, 8.7577e-02, 1.7543e-01, 9.6019e-01,\n",
      "         1.9229e-01, 7.7817e-01, 4.6307e-01, 9.5810e-01, 7.0582e-01, 2.1488e-01,\n",
      "         2.6794e-02, 9.9689e-01, 8.2776e-01, 8.5164e-01, 8.4272e-01, 7.4476e-01,\n",
      "         6.6411e-01, 8.7023e-02, 1.6554e-01, 9.8538e-01, 3.2673e-01, 3.5328e-04,\n",
      "         9.6867e-01, 5.2756e-01, 5.9978e-01, 1.0616e-01, 8.6720e-01, 5.5322e-01,\n",
      "         4.1612e-02, 3.8194e-01, 3.2962e-01, 9.8274e-01, 8.4231e-01, 2.2083e-01,\n",
      "         8.1171e-01, 8.3744e-01, 1.5821e-01, 9.8731e-02, 4.2670e-01, 5.2575e-01,\n",
      "         8.3331e-01, 1.4784e-01, 9.9107e-03, 4.5482e-01, 8.1671e-01, 3.8844e-03,\n",
      "         2.3174e-01, 7.4458e-01, 7.7429e-01, 4.2282e-01, 4.2889e-01, 4.3399e-01,\n",
      "         4.8397e-01, 4.1395e-01, 9.0431e-01, 8.0041e-01, 3.4602e-01, 5.6716e-03,\n",
      "         6.2812e-01, 6.4776e-01, 3.5146e-01, 1.9940e-02, 1.8077e-01, 7.5336e-01,\n",
      "         2.8737e-01, 6.3169e-01, 6.8965e-01, 1.1461e-01, 2.0218e-02, 6.4073e-01,\n",
      "         6.5257e-01, 4.0392e-01, 2.8070e-02, 8.2334e-01, 7.0368e-01, 3.0179e-01,\n",
      "         7.5133e-01, 5.9815e-01, 8.6875e-02, 2.9556e-01, 7.7311e-01, 5.1375e-01,\n",
      "         6.5688e-01, 1.2354e-01, 7.8663e-01, 5.6662e-01, 4.7691e-02, 7.3760e-01,\n",
      "         9.8583e-02, 7.7524e-01, 5.7110e-01, 9.6139e-01, 8.7747e-02, 2.8783e-01,\n",
      "         6.5822e-01, 5.1409e-01, 1.0756e-01, 3.3482e-01, 9.0631e-01, 4.1449e-01,\n",
      "         8.2712e-01, 7.7040e-01, 1.4082e-01, 4.9429e-01, 4.8979e-01, 9.6402e-01,\n",
      "         5.7208e-01, 4.0620e-02, 1.3872e-01, 5.4721e-01, 2.9480e-01, 8.0123e-01,\n",
      "         8.9841e-01, 2.0090e-01, 6.5729e-01, 9.9834e-01, 4.1028e-01, 4.7038e-03,\n",
      "         4.9526e-01, 4.4315e-01, 6.0864e-01, 5.6539e-01, 7.4527e-01, 9.8322e-01,\n",
      "         1.7652e-01, 9.4095e-01, 4.7036e-01, 3.6760e-01, 2.4399e-01, 6.8974e-01,\n",
      "         3.6665e-02, 1.3717e-01, 5.3580e-01, 5.2601e-01, 4.5989e-01, 1.4694e-01,\n",
      "         7.7578e-01, 2.3755e-01, 5.6247e-01, 7.6578e-01, 3.2984e-01, 2.0435e-01,\n",
      "         7.6942e-01, 1.5078e-01, 1.5257e-01, 2.3382e-01, 7.8772e-01, 4.7570e-01,\n",
      "         5.8300e-01, 9.0889e-01, 5.8683e-01, 5.9815e-01, 1.0165e-01, 7.8811e-01,\n",
      "         9.4711e-01, 6.7165e-01, 6.2866e-01, 5.3192e-01, 6.8051e-01, 3.9161e-01,\n",
      "         2.8630e-01, 3.1192e-01, 6.2192e-02, 6.3088e-01, 9.6658e-01, 3.2994e-01,\n",
      "         3.2553e-01, 1.0051e-01, 2.4547e-01, 7.7056e-01, 5.5266e-01, 5.9724e-01,\n",
      "         1.3028e-01, 6.7515e-01, 4.3138e-01, 3.8567e-01, 9.5354e-01, 6.6726e-01,\n",
      "         4.0682e-01, 7.6290e-01, 9.7958e-01, 7.2927e-01, 1.6390e-01, 5.7664e-01,\n",
      "         5.6209e-01, 7.6265e-01, 2.7408e-01, 4.7080e-01, 8.9468e-01, 6.0285e-01,\n",
      "         9.3047e-01, 7.7201e-02, 3.0453e-01, 5.9606e-01, 2.4499e-01, 4.0885e-01,\n",
      "         5.9660e-01, 8.9761e-01, 3.2927e-02, 2.2542e-01, 9.7769e-02, 9.9043e-01,\n",
      "         1.3669e-01, 3.2687e-01, 7.5020e-01, 1.9209e-01, 2.3066e-01, 8.9456e-01,\n",
      "         3.6813e-01, 7.8852e-01, 5.2982e-01, 4.0033e-01, 5.1679e-01, 8.8967e-01,\n",
      "         6.8979e-01, 3.1583e-01, 5.2633e-01, 2.1072e-01, 2.0588e-01, 7.8914e-01,\n",
      "         3.0046e-01, 8.3898e-02, 8.5096e-01, 5.3476e-01, 6.9120e-01, 4.7078e-01,\n",
      "         2.1035e-01, 2.9241e-01, 6.9594e-01, 6.8591e-01, 2.0426e-01, 9.3517e-01,\n",
      "         5.8343e-01, 1.2535e-02, 3.0535e-01, 7.4556e-02, 2.6170e-01, 2.3607e-02,\n",
      "         3.6685e-01, 1.4786e-01, 6.0632e-01, 2.9775e-01, 7.7901e-01, 7.2849e-01,\n",
      "         9.1980e-01, 3.6239e-01, 7.6248e-01, 4.7464e-01, 5.7046e-02, 9.6632e-01,\n",
      "         2.2128e-01, 6.6985e-02, 4.1898e-01, 6.1647e-01, 9.6774e-01, 1.7945e-01,\n",
      "         9.2309e-01, 3.6501e-01, 8.7282e-01, 7.0095e-01, 5.5669e-01, 5.6435e-01,\n",
      "         2.8203e-01, 9.7546e-01, 1.3125e-01, 4.3516e-01, 1.2801e-01, 2.3958e-01,\n",
      "         7.2495e-01, 6.8361e-01, 8.2773e-01, 6.1236e-01, 7.8692e-01, 4.2571e-01,\n",
      "         8.7673e-01, 9.2423e-01, 5.4736e-01, 9.3999e-02]])\n"
     ]
    }
   ],
   "source": [
    "rand_t = torch.rand((28, 28))\n",
    "rand_t = rand_t.view(1, 28 * 28)\n",
    "print(rand_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(rand_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0634, -0.0735, -0.1143,  0.1129,  0.0659,  0.1179,  0.1769, -0.0504,\n",
       "          0.0133,  0.1641]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
